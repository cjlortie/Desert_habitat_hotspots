---
title:
author: "cjlortie, afilazzola, rdaigle"
date: "2018"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
##A contrast of occupancy models for endangered species
<br>

###Purpose
To explore joint distribution models for endangered animal species listed within the San Joaquin Desert. Individual species occurrence data are typically scraped from gbif then Maxent models are used to predict and explore occupancy patterns include inference of niche. More than one species not tested concurrently within a region. [Hierarchical Modelling of Species Communities (HMSC)](https://onlinelibrary.wiley.com/doi/full/10.1111/ele.12757) is major innovation that can explore a measure of association amongst species in a region. A [recent package for R](https://github.com/guiblanchet/HMSC) has provided the means to directly apply this novel framework to occurrence data.


<br>
![](./model.jpg)

<br>
[ecoblender](http://ecoblender.org)
<br>

###Data
Select all federally listed endangered species within the San Joaquin Desert.

```{r load packages,message=FALSE}
require(rgbif)
require(sdmpredictors)
require(HMSC)
require(sf)
require(tidyverse)
require(ggplot2) # must have geom_sf(), may need to devtools::install_github("tidyverse/ggplot2") 
```

Load the desert of interest.

```{r}
SJD <- st_read("data/shapefile/SanJoaquinDesert_FINAL.shp") %>% 
  st_transform("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

ggplot(SJD) +
  geom_sf()
```
We use the rgbif package to download the data. We only need to run the `occ_download` command once since the query is stored on the GBIF website. This command doesn't actually work, on GBIF it says "The download request was unsuccessful. Please try it again or get in touch. Contact helpdesk"

To make it work in a round-about way, go to GBIF, rerun the query created by the command below and download that data, it will successfully create a new query that works!

*note: the keys will not match!*


```{r,eval=FALSE}
gbif <- occ_download(paste0("geometry =", st_as_text(st_convex_hull(SJD$geometry))),user =gbif_user,pwd = gbif_pwd,email = gbif_email)
```

Then download the data and load it.

```{r}
key <- "0017562-180508205500799" #from the manual rerun query on gbif
gbif_citation(x=occ_download_meta(key))

if(!file.exists(file.path("data",paste0(key,".zip")))){
  occ_download_get(key=key,path="data")
}

SJD_gbif <- occ_download_import(key=key,path="data",fill=FALSE)


```
To do any SDM'ing we need environmental data, and if we're inerested in climate, we need to have layers that are available now and in the future

```{r}
layers_future <- list_layers_future(terrestrial=TRUE) 
layers <- list_layers(terrestrial=TRUE) %>% 
    filter(layer_code %in% layers_future$current_layer_code)

# TODO
# I'm arbitratily selecting the first 10 layers, you will want to give this more though (e.g. ipcc scenarions, more important variables, etc)
layers <- layers[1:10,]

#make a lat long version of SJD
SJD_ll <- SJD %>%
    st_transform("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0") %>%
    as("Spatial")

# download/load layers
enviro_raster <- load_layers(layers$layer_code,datadir=paste0(getwd(),"/data/SDM/")) %>% 
    crop(SJD_ll) %>% 
    mask(SJD_ll)

# make it sf style
raster_grid <- rasterToPolygons(enviro_raster) %>%
  st_as_sf() %>%
  st_transform(st_crs(SJD))

enviro_raw <- raster::extract(enviro_raster, raster_grid, fun = mean, na.rm = T)


plot(enviro_raster)
```



```{r}

enviro_raster_future <- load_layers(layers$layer_code,datadir=paste0(getwd(),"/data/SDM/")) %>% 
    crop(SJD_ll) %>% 
    mask(SJD_ll)

enviro_raw_future <- raster::extract(enviro_raster_future, raster_grid, fun = mean, na.rm = T)

plot(enviro_raster_future)

```

```{r}

# Overlay occurrences on grid
sp_abun_raw <- SJD_gbif  %>%
  filter(!is.na(species),species!="") %>%
  st_as_sf(coords = c("decimallongitude", "decimallatitude"),
                 crs="+proj=longlat +datum=WGS84")  %>%
  st_transform(st_crs(SJD)) %>% 
  mutate(gridID=st_covered_by(.,raster_grid)) %>%
  mutate(gridID=factor(sapply(gridID,FUN=function(x) ifelse(length(x)==0,NA,x[[1]])),
                       levels=row.names(raster_grid))) %>% 
  data.frame() %>% 
  group_by(gridID,species) %>%
  summarize(records=n()) %>%
  spread(species,records,fill=0) %>% 
  filter(!is.na(gridID)) %>% 
  left_join(data.frame(gridID=row.names(raster_grid)),., by = "gridID") %>%
  data.frame() %>% 
  column_to_rownames("gridID") %>% 
  as.matrix()

sp_abun_raw[is.na(sp_abun_raw)] <- 0    



sp_pres_raw <- (sp_abun_raw>0) %>%
  data.frame() %>%
  sapply(as.numeric)
```


```{r}
# Creating HMSC dataset for analyses
HMSCdata <- as.HMSCdata(Y = sp_abun_raw,
                        X = enviro_raw,
                        Random = data.frame(sampling_unit = row.names(raster_grid)))
memory.limit(65216)
gc()
model <- hmsc(HMSCdata)
```

```{r}
# Generate predictions
HMSCpred <- cbind(raster_grid,predict(model))

# future
HMSCdata_future <- as.HMSCdata(Y = sp_abun_raw,
                        X = enviro_raw_future,
                        Random = data.frame(sampling_unit = row.names(raster_grid)))

HMSCpred_future <- cbind(raster_grid,predict(model,newdata=HMSCdata_future))
```

